{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - SQL, NoSQL and Data Warehouses\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Tutorial will take place online on Friday, Nov 27th 2020 at 11:00am (CET) c.t. <br><br>This Assignment is due on Monday, Dec 7th 2020 at 11:59pm (CET)<br>The next Assignment (Assignment 2) will be published on Dec 15th (so there is a one week break)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Total Points | Points Needed To Pass (50%) | Points Reached |\n",
    "|:-:|:-:|:-----------:|\n",
    "| 10 | 5 |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello there! This is the first (actual) Assignment for Big Data Management and Analytics this year. Here, we will take a practical look at databases as well as data warehouses. In this assignment you will also find the first graded <font color='#782769'>TODOs</font>, which are <font color='#e52425'>clearly marked</font> and are mandatory to submit to take part in the exam, so please read the assignment carefully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will work at a fictonal shop, selling bikes via a website. Your boss first tasks you with setting up a database for all your stored information but as bosses do, he might change his mind about what we wants a couple of times on the way. At the end of this Assignment you should have a better understanding of how different structures of databases work and what their respective shortcomings or strenghts are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are mainly working on databases now, you need some software to manage and set up a database on your local PC. For this we will use PostgreSQL (often pronounced \"Postgres\"). PostgreSQL is a free, open-source database management tool that you can download from [their website](https://www.postgresql.org/download/).<br>\n",
    "After downloading please install it and its necessary components. **During the installation you will be asked to set a master password, make sure to remember it**. When the Setup is complete you can open \"pgAdmin 4\", which opens up your browser with a window similar to this:\n",
    "<img src=\"media/Screenshot_pgAdmin.png\" height=600>\n",
    "Now, open up your PostgreSQL 13 Server with the small arrow next to \"Servers\" until you can see the \"postgres\" sample database. Right-click on \"Databases\" to create a new one, we will call \"bikeDB\". If you click on it, your left side should look something like this:\n",
    "<img src=\"media/Screenshot_pgAdmin1.png\" height=600>\n",
    "Next, right-click again onto your \"bikeDB\" and choose \"CREATE Script\" from the menu. You should now see a workspace similar to this:\n",
    "<img src=\"media/Screenshot_pgAdmin2.png\" height=600>\n",
    "You are now ready to go to work on your (yet to be filled) database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Creating a Relational Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, you are working for a company that sells bikes online. You are new at the company but has been tasked with setting up a database to gain more insights about the company's recent sales. <br> \n",
    "The first thing you need to do is to get familiar with the data as well as the relations between the different entities. <br>*(As we hope, you have some experience with SQL and databases in general, we will not go into detail here but we will do a recap in the Tutorial, so watch the recording if you missed it and need a refresh!)* <br> One of your collegues can provide you with an Entity Relationship Diagram to get a first overview:\n",
    "<img src=\"media/ER.png\" width=600 height=600>\n",
    "Lucky for you, the scripts for creating the tables can still be found somewhere, so you don't have to start from the scratch at least. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pgAdmin, use the \"Open File\" button in the top left corner (below \"Dashboard\") to open up the **CREATEbikeDB.sql** file into your Query Editor (you can also drag and drop it). Take a closer look at the first lines, especially the CREATE TABLE statements to get familiar with datatypes and again the overall structure of the database. <br> Then execute the query with F5 or the button on top. You should get a few warnings but other than that, your database should be good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Including Data from other Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to make sure that all tables are present and all data was inserted correctly. For this we are gonna start with a simple query that will return the first $n$ entries of a table. In pgAdmin open the \"query tool\", that can be found in the top left corner (below Dashboard) or by clicking on \"Query Tool\" in the \"Tools\" menu section on top. You should then get an empty window, where you can type your SQL Query and again execute it by pressing F5 (or the button)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "--- This is a simple SQL statement to get the first 30 entries from the table article\n",
    "--  This will not execute here in Jupyter but you will need to copy it into pgAdmin 4\n",
    "\n",
    "SELECT * FROM public.article -- public.orders, public.orderposition, public.articlereservation\n",
    "ORDER BY 1 ASC \n",
    "LIMIT 30;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to try out some more small queries until you get a good feel of the data. <br><br>\n",
    "You then present the database to your boss to what he tells you that there is more data, that he would like to include. This new data was not stored in a database before, so the data is stored in csv, json and xml files. Now your boss wants you to include these files into your existing database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#782769'>Now it's your turn!<br> TODO #1:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the files before insertion into the tables Orders, Orderposition and Articlereservation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a small python script which will parse and include the data stored in three different kind of files all located in the folder **data_to_stage** (articlereservation.csv, ORDER.csv, ORDER_10.json to ORDER_13.json and ORDER_1.xml to ORDER_9.xml - *total of 15 files*) into your bikeDB.\n",
    "\n",
    "Make sure you pay attention to the relations within the data, so you create no open and dangling references! *(Hint: You should first insert into Orders)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       ORDERPOSITION\n",
      "0  {'@POSID': '1', 'QUANTITY': '11', 'ARTID': '20...\n",
      "1  {'@POSID': '2', 'QUANTITY': '12', 'ARTID': '50...\n",
      "2  {'@POSID': '3', 'QUANTITY': '13', 'ARTID': '20...\n",
      "3  {'@POSID': '4', 'QUANTITY': '8', 'ARTID': '100...\n",
      "                                       ORDERPOSITION\n",
      "0  {'@POSID': '1', 'QUANTITY': '11', 'ARTID': '20...\n",
      "1  {'@POSID': '2', 'QUANTITY': '12', 'ARTID': '50...\n",
      "2  {'@POSID': '3', 'QUANTITY': '13', 'ARTID': '20...\n",
      "3  {'@POSID': '4', 'QUANTITY': '8', 'ARTID': '100...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     2002-01-02\n",
       "1     2002-01-03\n",
       "2     2002-01-03\n",
       "3     2002-01-03\n",
       "4     2002-01-03\n",
       "5     2002-01-03\n",
       "6     2002-01-03\n",
       "7     2002-01-03\n",
       "8     2002-01-03\n",
       "9     2002-01-03\n",
       "10    2002-01-03\n",
       "11    2002-01-03\n",
       "12    2002-01-03\n",
       "13    2002-01-04\n",
       "14    2002-01-04\n",
       "15    2002-01-04\n",
       "16    2002-01-04\n",
       "17    2002-01-04\n",
       "18    2002-01-04\n",
       "19    2002-01-04\n",
       "20    2002-01-04\n",
       "21    2002-01-04\n",
       "22    2002-01-04\n",
       "23    2002-01-05\n",
       "24    2002-01-05\n",
       "25    2002-01-05\n",
       "26    2002-01-05\n",
       "27    2002-01-05\n",
       "28    2002-01-05\n",
       "29    2002-01-05\n",
       "30    2002-01-05\n",
       "31    2002-01-05\n",
       "32    2002-01-05\n",
       "33    2002-01-05\n",
       "34    2002-01-05\n",
       "35    2002-01-05\n",
       "Name: ORDERDATE, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check which files need to be staged!\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import json\n",
    "from functools import reduce\n",
    "from parse import *\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas_read_xml as pdx\n",
    "from pandas_read_xml import auto_separate_tables\n",
    "\n",
    "file_name = os.listdir('./data_to_stage')\n",
    "csv_df = list()\n",
    "xml_df = list()\n",
    "json_df = list()\n",
    "date_format = '%Y-%m-%d'\n",
    "for file in file_name:\n",
    "    if file.endswith(\".csv\"): #if the file is .csv file\n",
    "        df_csv = pd.read_csv(os.path.join('./data_to_stage',file)) #pd.read_csv('abc.csv')\n",
    "        csv_df.append(df_csv)  \n",
    "    if file.endswith(\".xml\"): #if the file is .xml file \n",
    "        df_xml = pdx.read_xml(os.path.join('./data_to_stage',file), ['ORDER', 'ORDERPOSITIONS'])\n",
    "        xml_df.append(df_xml)\n",
    "    if file.endswith(\".json\"): #if the file is .json file\n",
    "        df_json = pd.read_json(os.path.join('./data_to_stage',file),orient='index')\n",
    "#         df1 = df_json.drop('ORDERPOSITIONS',axis=1)\n",
    "#         df2 = json_normalize(df_json['ORDERPOSITIONS'])\n",
    "#         df = df1.merge(df2,left_index=True,right_index=True)\n",
    "#         res = pd.json_normalize(df_json)\n",
    "        json_df.append(df_json)\n",
    "\n",
    "# print(json_df[0])\n",
    "print(xml_df[0])\n",
    "new = pd.DataFrame(xml_df[0])\n",
    "print(new)\n",
    "\n",
    "# #change date_time format\n",
    "def convert_formats_to_datetimes(col, formats):\n",
    "    out = [pd.to_datetime(col, format=x, errors='coerce',utc= True).dt.date for x in formats]\n",
    "    return reduce(lambda l,r: pd.Series.combine_first(l,r), out)\n",
    "# print(csv_df[1]['ORDERDATE'])\n",
    "formats = ['%a, %d %b, %Y','%Y-%m-%d','%a,%d/%m/%y,%I:%M%p','%B %d, %Y' ,'%Y-%m-%dT%H:%M:%S%z']#but this one is not dynamic way.\n",
    "\n",
    "csv_df[1]['ORDERDATE'] = csv_df[1]['ORDERDATE'].pipe(convert_formats_to_datetimes, formats)\n",
    "# print(csv_df[1]['ORDERDATE'])\n",
    "\n",
    "\n",
    "# FIELDS=['POSTID','ORDID','ARTID','QUANTITY','TOTALPRICE']\n",
    "# def clean_data(data):\n",
    "#     table = pd.DataFrame()\n",
    "#     for i in range(len(data) - 1):\n",
    "#         df = pd.json_normalize(data[i + 1])\n",
    "#         df = df[FIELDS]\n",
    "#         table = table.append(df)\n",
    "#     return table \n",
    "csv_df[1]['ORDERDATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure that field formats from the files match the given field formats from the database tables before enhancing the database tables with the new data from the files.\n",
    "*(Hint: There are many useful libraries for handling csv, xml, json and datetime)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO#1:\n",
    "#  Get creative on how you want to preprocess the data in the different files.\n",
    "# print(json_df[0])\n",
    "# result = json_normalize(json_df[0])\n",
    "# print(json_df[0]['CUSTID'])\n",
    "# print(json_df[0]['ORDERPOSITIONS.QUANTITY'])\n",
    "# from pandas.io.json import json_normalize\n",
    "\n",
    "# data = json_df[0]\n",
    "# FIELDS=['POSTID','ORDID','ARTID','QUANTITY','TOTALPRICE']\n",
    "# pd.json_normalize(data)\n",
    "# for i in range(len(data) - 1):\n",
    "#     df = pd.json_normalize(data[i + 1])\n",
    "#     df = df[FIELDS]\n",
    "#     table = table.append(df)\n",
    "# result = json_normalize(data, 'ORDERPOSITION', FIELDS)\n",
    "# result = json_normalize(data)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#782769'>Now it's your turn!<br>TODO #2:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in c:\\users\\schueler\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (20.2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\schueler\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.8.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## Maybe you need to install the PostgreSQL database adapter first:\n",
    "%pip install --upgrade pip\n",
    "%pip install psycopg2\n",
    "#or on unix: !sudo apt install python3-psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After proprocessing our data, we need a script to INSERT the data into our existing tables in the Postgres database.\n",
    "\n",
    "For that we first establish the connection and then write a loop which fills in the preprocessed data into an INSERT statement. This statement will then be sent to and executed by our database system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n",
      "Error: duplicate key value violates unique constraint \"pk_articlereservation\"\n",
      "DETAIL:  Key (posid, artid)=(302, 500002) already exists.\n",
      "\n",
      "201\n",
      "Error: duplicate key value violates unique constraint \"pk_articlereservation\"\n",
      "DETAIL:  Key (posid, artid)=(201, 100002) already exists.\n",
      "\n",
      "402\n",
      "Error: duplicate key value violates unique constraint \"pk_articlereservation\"\n",
      "DETAIL:  Key (posid, artid)=(402, 500001) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(14) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(15) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(16) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(16) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(16) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(17) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(17) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(18) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(18) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(18) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(19) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(19) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(20) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(21) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(22) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(22) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(22) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(23) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(24) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(25) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(25) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(25) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(25) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(26) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(27) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(27) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(27) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(27) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(28) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(28) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(29) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(29) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(29) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(29) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(30) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"pk_order\"\n",
      "DETAIL:  Key (ordid)=(30) already exists.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "## Establishing the connection to your database\n",
    "conn = psycopg2.connect(host='localhost',  # since you installed your PostgresDB locally \n",
    "                        port=5432,         # default PostgresDB port\n",
    "                        user='postgres',   # the db user you used for database creation\n",
    "                        password='1261993',       # TODO#2.1: Fill in the db password you used for database creation\n",
    "                        database='bikeDB') # the name of the database you created\n",
    "\n",
    "#Thanks to https://naysan.ca/2019/11/02/from-pandas-dataframe-to-sql-table-using-psycopg2/\n",
    "def single_insert(conn, insert_req):\n",
    "    \"\"\" Execute a single INSERT request \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(insert_req)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    cursor.close()\n",
    "\n",
    "# Inserting each row\n",
    "for i in csv_df[0].index:\n",
    "    print(csv_df[0]['posid'][i])\n",
    "    query = \"\"\"\n",
    "            INSERT INTO articlereservation(posid, artid, artcount)\n",
    "            VALUES(%s, %s, %s)\"\"\" %(csv_df[0]['posid'][i],csv_df[0]['artid'][i],csv_df[0]['artcount'][i])\n",
    "    single_insert(conn, query)\n",
    "for i in csv_df[1].index:\n",
    "    # add '%s'::date for inserting date from dataframe to sql\n",
    "    query = \"\"\"\n",
    "            INSERT INTO orders(ordid, orderdate, custid, staffid)\n",
    "            VALUES(%s, '%s'::date, %s, %s)\"\"\" %(csv_df[1]['ORDID'][i],csv_df[1]['ORDERDATE'][i],csv_df[1]['CUSTID'][i],csv_df[1]['STAFFID'][i])\n",
    "    query_2 = \"\"\"INSERT INTO orderposition(posid, ordid, artid, quantity, totalprice)\n",
    "                 VALUES(%s, %s, %s, %s, %s)\"\"\"%(csv_df[1]['POSID'][i],csv_df[1]['ORDID'][i],csv_df[1]['ARTID'][i],\n",
    "                                               csv_df[1]['QUANTITY'][i],csv_df[1]['TOTALPRICE'][i])\n",
    "    single_insert(conn, query)\n",
    "    single_insert(conn, query_2)\n",
    "\n",
    "## TODO#2.2: Write the code for inserting the prepared data into the database\n",
    "#  <table_name> has to be replaced with the table you want to insert into\n",
    "#  you can state the order of the column names at (<col_name1, ...)\n",
    "#  and the (v1, v2, ...) has to be replaced with your preprocessed data\n",
    "#  Note, that you also have to add %s into the string to match your number of arguments\n",
    "\n",
    "\n",
    "\n",
    "# conn.commit() # <- We MUST commit to reflect the inserted data\n",
    "\n",
    "# cursor.close()\n",
    "\n",
    "# when a databse or cursor connection has been opened it also needs to be closed again\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works fine, you should now have a database with 50000 orderposition entries, you can check that by right-clicking on the table in pgAdmin and selecting \"Count Rows\". A small alert in the bottom right corner pops up to tell you the number of entries! Or you can of course use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT COUNT(*) FROM public.orderposition\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Querying a Relational Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#782769'><br>Now it's your turn!<br>TODO #3:</font>\n",
    "### <font color='#e52425'>This will be graded. You can reach a total of 2+2+2=6 points</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as you tell your boss you integrated all the data, he asks you some questions.\n",
    "\n",
    "Write an SQL Query for every question your boss askes you. You can test them in pgAdmin and copy the final version into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- TODO#3.1 (2 points): \"Show me all customers, that are situated in Munich!\"\n",
    "\n",
    "SELECT *\n",
    "FROM customer\n",
    "WHERE place = 'Munich'\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- TODO#3.2 (2 points): \"What is (are) the most expensive bike(s) that we shipped out?\"\n",
    "\n",
    "SELECT aid, name \n",
    "FROM article\n",
    "WHERE aid = (SELECT aid FROM shipment WHERE netprice = (SELECT MAX(netprice) FROM shipment))\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- TODO#3.3 (2 points): \"What do customers pay on average? (Take the average price over all orders.)\"\n",
    "#avg price of all customers pay\n",
    "SELECT AVG(totalprice)\n",
    "FROM orderposition\n",
    "#avg price one customer pays\n",
    "SELECT avg(orderposition.totalprice), customer.cid, customer.name\n",
    "FROM orderposition\n",
    "LEFT JOIN orders\n",
    "ON orderposition.ordid = orders.ordid\n",
    "INNER JOIN customer\n",
    "ON orders.custid = customer.cid\n",
    "GROUP BY customer.cid\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Creating a Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as you thought you were finally done with the database, your boss comes back from a big Buzzword-Conference and tells you, that he heard about Data Warehouses and that they are way quicker and cooler than relational databases! You could get even more insights as they have cool features like putting data in a cube or something... (He didn't quite got that part).\n",
    "\n",
    "So, of course, he wants you to build one of these Warehouses now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the file **CREATEwarehouse.sql** you will find the DDL-CREATE statements for the star-structure our data warehouse will have. (Check out the lecture for the theoretical part on that.) Again, right click on you bikeDB and select CREATE SCRIPT and copy the statements or open the file to add these four tables to your database. (Please ADD them to your existing schema, as we still need the old tables to fill the new ones!)\n",
    "\n",
    "Make yourself familiar with the new structure. You can also create a small ER diagram on paper around the Facttable (like in the lecture shortly mentioned) for a better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to fill the warehouse with our data. For that, we gave you a file called **procedure.sql**. This procedure will copy the data from the old tables into our new warehouse schema. \n",
    "\n",
    "In pgAdmin look for the \"Procedures\" tab under \"Schemas\" in your bikeDB. It should look pretty much like this (except you don't have a procedure created yet):\n",
    "<img src=\"media/Screenshot_pgAdmin3.png\" height=600>\n",
    "Now, right-click on \"Procedures\" and create a new one. Give it a name like \"fill-warehouse\" and select \"plpgsql\" as \"Language\" in the \"Definition\" Tab. Then switch to the \"Code\" Tab and copy the code from the **procedure.sql** file and save your procedure.\n",
    "\n",
    "Now it should show up in the list on the left. As we just created the procedure, we also need to execute it. For that right-click on the \"fill_warehouse\" procedure and select \"Scripts > EXEC Script\" or you can use \n",
    "```sql\n",
    "CALL public.fill_warehouse()\n",
    "```\n",
    "in the Query Editor. Now check if your new tables are filled with data by either counting rows or querying the first few rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Querying a Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries for data warehouses are not that different from the SQL queries you are used to. For one, they usually perform quicker on huge amount of data (which is hard to show here^^) because they avoid joins and they can be used to get a more general overview or a more detailed look into certain aspects of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#782769'>Now it's your turn!<br>TODO #4:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an SQL query which only uses your Facttable and the new warehouse-tables (dcustomer, ddate and dproduct), which will show you all sold equipment between the week 201047 (week 47 of 2010) and 201050 per equipment and week.\n",
    "\n",
    "```sql\n",
    "--- TODO#4:\n",
    "\n",
    "SELECT  COALESCE(TO_CHAR(P.ArtId, '999999'), '-SUM-') As ArtID, \n",
    "        COALESCE(P.Name, '---SUM---') As Name,\n",
    "        COALESCE(TO_CHAR(WeekInYear, '999999'), 'ALLWEEKS') As Week, \n",
    "        COALESCE(SUM(F.Quantity),0) AS Quantity\n",
    "FROM    Facttab F -- JOIN ...\n",
    "--      ...\n",
    "--      ...\n",
    "ORDER BY 1 ASC, 3 ASC;\n",
    "```\n",
    "\n",
    "Your result should look like this:\n",
    "<img src=\"media/WH.png\" height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Setting up a NoSQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though your boss is very happy with the new warehouse and all the information you can get from it with just a few lines of code, he still insists that there's got to be an even quicker way... He also tells you, that he read a little into data warehouses (so now he is of course an expert on it -.-) and figured out that data warehouses are \"sooooo 1980s\" and \"the company needs to stay fresh and keep up with the trends...\" (You also kinda expect him to use either the term Machine Learning of Artificial Intelligence next but he doesn't...yet.)\n",
    "\n",
    "So, of course you offer to migrate at least a part of your database to a NoSQL database. NoSQL is more suitable for flexible and expendable environments and is therefore used in companies that deal with a lot of (possibly changing) data, questionable for your bike shop, but hey, the boss said so... ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up (another) database environment. We are gonna use MongoDB for this example, which is one of the most frequent used, document-oriented database management systems. Please download the Community Version from [their website](https://www.mongodb.com/try/download/community) and install it on your computer. After installing, you might need to set the PATH variables in Windows. You can come ask us, if you have any troubles with that.\n",
    "\n",
    "If you are working on a Unix-System, you can find a detailed installation guide in the **mongoDB_on_Linus.txt** file.\n",
    "\n",
    "On Windows, mongoDB will automatically install \"mongoDB Compass\" which is a GUI for NoSQL databases. There are also a ton of tools, which help you to migrate a relational database into a non-relational document-oriented filesystem. If you are interested, here are a couple of useful links for further reading:\n",
    "* https://www.mongodb.com/try/download/compass # IDE for mongodb and migration tool for RDBMS to mongodb \n",
    "* https://robomongo.org/ (Robo 3T) # IDE for mongodb and migration tool for RDBMS to mongodb\n",
    "* https://github.com/gagoyal01/mongodb-rdbms-sync # migration tool for RDBMS to mongodb\n",
    "* https://github.com/compose/transporter # migration tool for RDBMS to mongodb \n",
    "* https://www.researchgate.net/project/Darwin-Schema-Management-in-NoSQL-Databases # migration tool for RDBMS to mongodb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn what the tools are doing you will implement the migration partially from scratch.\n",
    "\n",
    "Therefore, the following lines show you the setup and basic functionalities of the mongodb adapter pymongo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\schueler\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.11.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "### install mongodb adapter pymongo:\n",
    "%pip install pymongo --no-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'bikeDB')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First, we declare(!) a new database called bikeDB\n",
    "#  We can also check if the connection works (connect=True)\n",
    "\n",
    "db = client.bikeDB\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'bikeDB'), 'article')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Next, we declare(!) a collection of files (called article)\n",
    "#  Remember, NoSQL in general is document based, so we are working on \n",
    "#  a number of different files not on tables anymore.\n",
    "#  So what used to be tables are now collections.\n",
    "\n",
    "collection = db.article\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's take a look at what is inside of the collection.\n",
    "\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is nothing. Why is that the case?\n",
    "An important note about collections (and databases) in MongoDB is that they are created lazily - none of the above commands have actually performed any operations on the MongoDB server. Collections and databases are created when the first document is inserted into them.\n",
    "So let's do that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObjectId('5fbd13e735c6ccf8ff97eb2a')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_collection_documents = []\n",
    "\n",
    "## As a Reminder, the columns of old table article: \n",
    "#    aid, name, net, tax, price, color, description, measure, made\n",
    "\n",
    "#  This is a sample document\n",
    "dict_article_document = {\n",
    "    #\"_id\": 100001, # sets e.g. aid as documentid instead of the auto-created ObjectId(\"5fb7000f0a2b90a2af21f7a8\")\n",
    "    \"aid\": 100001,\n",
    "    \"name\": \"Man City Bike\",\n",
    "    \"net\": 588.24,\n",
    "    \"tax\": 111.76,\n",
    "    \"price\": 700.00,\n",
    "    \"color\": 'blue',\n",
    "    \"description\": '26 inch',\n",
    "    \"measure\": 'P',\n",
    "    \"made\": 'E'\n",
    "}\n",
    "\n",
    "#  Declares collection article, same as above\n",
    "article_collection = db.article \n",
    "\n",
    "#  Now we collect the created document(s) for bulk insert in list\n",
    "lst_collection_documents += [dict_article_document] \n",
    "\n",
    "## Adds all documents dict_article_document (row/tuple) from list lst_collection_documents \n",
    "#  to collection article_collection via bulk insert command \"insert_many\"\n",
    "result = article_collection.insert_many( lst_collection_documents ) \n",
    "result.inserted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['article']\n",
      "db: admin\n",
      "db: bikeDB\n",
      "db: config\n",
      "db: local\n"
     ]
    }
   ],
   "source": [
    "## Now after the first document insertion the collection article as well as \n",
    "#  the database bikeDB get visible \n",
    "print(db.list_collection_names())\n",
    "for dba in client.list_databases():\n",
    "    print(f\"db: {dba['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#782769'>Now it's your turn!<br>TODO #5:</font>\n",
    "### <font color='#e52425'>This will be graded. You can reach a total of 1+2+1=4 points</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can see a prepared script to add the collections for *orders*, *orderposition*, *staff* and *article* into the MongoDB database. First we load the data from the files, you can find in the folder called **data_nosql** then we declare the collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer.csv', 'article.csv', 'orderposition.csv', 'staff.csv', 'orders.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## These are the files that need to be imported\n",
    "\n",
    "import os\n",
    "data_path = './data_nosql'\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The declaration for the collection for *customer* is missing. Try to understand the script and add the *customer* collection declaration.\n",
    "\n",
    "Further down in the *get_collection_document_list* function, there is also a loop missing which iterativly created your documents and adds them to the list we want to insert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- orders ----\n"
     ]
    },
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "localhost:27017: [Errno 111] Connection refused, Timeout: 30s, Topology Description: <TopologyDescription id: 5fcd963fab929bfd3384c713, topology_type: Single, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused')>]>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3c98ddc86b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mmigrate_bikedb_from_rdbms_to_mongodb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-3c98ddc86b28>\u001b[0m in \u001b[0;36mmigrate_bikedb_from_rdbms_to_mongodb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# clean before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# create collection content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m   1125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_concern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             self.read_concern)\n\u001b[0;32m-> 1127\u001b[0;31m         \u001b[0mdbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     def _delete(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymongo/database.py\u001b[0m in \u001b[0;36mdrop_collection\u001b[0;34m(self, name_or_collection, session)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_purge_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket_for_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m             return self._command(\n\u001b[1;32m    923\u001b[0m                 \u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_socket_for_writes\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_socket_for_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwritable_server_selector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_select_server\u001b[0;34m(self, server_selector, session, address)\u001b[0m\n\u001b[1;32m   1276\u001b[0m                                         % address)\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m                 \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_selector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m                 \u001b[0;31m# Pin this session to the selected server if it's performing a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m                 \u001b[0;31m# sharded transaction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymongo/topology.py\u001b[0m in \u001b[0;36mselect_server\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    239\u001b[0m                       address=None):\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"\"\"Like select_servers, but choose a random server if several match.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         return random.choice(self.select_servers(selector,\n\u001b[0m\u001b[1;32m    242\u001b[0m                                                  \u001b[0mserver_selection_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                                                  address))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymongo/topology.py\u001b[0m in \u001b[0;36mselect_servers\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             server_descriptions = self._select_servers_loop(\n\u001b[0m\u001b[1;32m    200\u001b[0m                 selector, server_timeout, address)\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymongo/topology.py\u001b[0m in \u001b[0;36m_select_servers_loop\u001b[0;34m(self, selector, timeout, address)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;31m# No suitable servers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 raise ServerSelectionTimeoutError(\n\u001b[0m\u001b[1;32m    216\u001b[0m                     \u001b[0;34m\"%s, Timeout: %ss, Topology Description: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                     (self._error_message(selector), timeout, self.description))\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: localhost:27017: [Errno 111] Connection refused, Timeout: 30s, Topology Description: <TopologyDescription id: 5fcd963fab929bfd3384c713, topology_type: Single, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused')>]>"
     ]
    }
   ],
   "source": [
    "## hint: to lookup different kinds of table relationships see http://learnmongodbthehardway.com/schema/schemabasics/\n",
    "\n",
    "import pandas as pd\n",
    "   \n",
    "def load_bikedb_rdbms_data():\n",
    "    \n",
    "    df_article = pd.read_csv(f\"{data_path}/article.csv\", sep=';')\n",
    "    ## TODO#5.1 (1 point): Load the csv file for the customers\n",
    "    df_customers = pd.read_csv(f\"{data_path}/customer.csv\", sep=';')\n",
    "    df_orderposition = pd.read_csv(f\"{data_path}/orderposition.csv\", sep=';')\n",
    "    df_orders = pd.read_csv(f\"{data_path}/orders.csv\", sep=';')\n",
    "    df_staff = pd.read_csv(f\"{data_path}/staff.csv\", sep=';')\n",
    "    \n",
    "    return {\n",
    "        \"orders\": {\n",
    "            \"df\": df_orders,\n",
    "            \"pk\": 'ordid',\n",
    "            \"fks\": [\n",
    "                {\n",
    "                    \"df_fk_table\": df_orderposition,\n",
    "                    \"fk_colname\": 'ordid',\n",
    "                    \"fk_pk_colname\": 'posid'        \n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"orderposition\": {\n",
    "            \"df\": df_orderposition,\n",
    "            \"pk\": 'posid',\n",
    "            \"fks\": []\n",
    "        },\n",
    "\n",
    "        ## TODO#5.2 (2 points): Add the customer collection here:\n",
    "        # \"customer: {...},\"\n",
    "      \"customer\": {\n",
    "            \"df\": df_customers,\n",
    "            \"pk\": 'cid',\n",
    "            \"fks\": [\n",
    "                {\n",
    "                    \"df_fk_table\": df_orders,\n",
    "                    \"fk_colname\": 'custid',\n",
    "                    \"fk_pk_colname\": 'ordid'        \n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "                \n",
    "        \n",
    "        \"staff\": {\n",
    "            \"df\": df_staff,\n",
    "            \"pk\": 'staffid',\n",
    "            \"fks\": [\n",
    "                {\n",
    "                    \"df_fk_table\": df_orders,\n",
    "                    \"fk_colname\": 'staffid',\n",
    "                    \"fk_pk_colname\": 'ordid'        \n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"article\": {\n",
    "            \"df\": df_article,\n",
    "            \"pk\": 'aid',\n",
    "            \"fks\": [\n",
    "                {\n",
    "                    \"df_fk_table\": df_orderposition,\n",
    "                    \"fk_colname\": 'artid',\n",
    "                    \"fk_pk_colname\": 'posid'        \n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def lookup_fks(df_fk_table, fk_colname, fk_value, fk_pk_colname):\n",
    "    \n",
    "    return list(set(df_fk_table.query(f\"{fk_colname} == {fk_value}\")[fk_pk_colname].tolist()))\n",
    "    \n",
    "\n",
    "def create_document(row, pk_colname, doc_id_field=None, fk_relations=[]):\n",
    "    dict_row = row\n",
    "    \n",
    "    # Ensure type dict\n",
    "    if type(row) != dict:\n",
    "        dict_row = row.to_dict()\n",
    "    \n",
    "    # Create foreign key list fields\n",
    "    for fk in fk_relations:\n",
    "        dict_row[f\"{fk['fk_pk_colname']}s\"] = lookup_fks(\n",
    "            fk['df_fk_table'], fk['fk_colname'], dict_row[pk_colname], fk['fk_pk_colname'])\n",
    "    \n",
    "    # Ensure that value of doc_id_field to _id document identifier (primary key)\n",
    "    if doc_id_field:\n",
    "        return {'_id': int(dict_row[doc_id_field]), **dict_row}\n",
    "    return {**dict_row}\n",
    "\n",
    "def get_collection_document_list(data):\n",
    "    \n",
    "    collection_document_list = []\n",
    "\n",
    "    ## TODO#5.3 (1 point): Write a loop that for each row in your dataframe,\n",
    "    #  creates a document and add that documents to the list defined above. \n",
    "    \n",
    "    #for ...\n",
    "    \n",
    "    \n",
    "        \n",
    "    return collection_document_list\n",
    "\n",
    "def migrate_bikedb_from_rdbms_to_mongodb():\n",
    "    \n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.bikeDB\n",
    "    \n",
    "    rdbms_data = load_bikedb_rdbms_data()\n",
    "    \n",
    "    \n",
    "    for table, data in rdbms_data.items():\n",
    "        print(f\" ---- {table} ----\")\n",
    "        \n",
    "        # clean before\n",
    "        collection = db[table]\n",
    "        collection.drop() \n",
    "        \n",
    "        # create collection content\n",
    "        collection = db[table]\n",
    "        collection_document_list = get_collection_document_list(data)\n",
    "        \n",
    "        # write collection content to mongodb\n",
    "        result = collection.insert_many(collection_document_list)         \n",
    "        print(result.inserted_ids)\n",
    "        \n",
    "\n",
    "migrate_bikedb_from_rdbms_to_mongodb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked out, you should now see a long list of all documents inserted into your NoSQL database!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Querying a NoSQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have now successfully migrated the bikeDB table selection to mongodb, your boss wants to have the same reports available as before. That way, you could theoretically also compare, if NoSQL databases are indeed quicker and \"cooler\".\n",
    "\n",
    "As an example, let's say we want to find all article IDs and names with a price higher than 600.0. In SQL this query would look like this:\n",
    "```sql\n",
    "SELECT aid, name\n",
    "FROM article\n",
    "WHERE price > 600.0;\n",
    "```\n",
    "\n",
    "When querying with NoSQL however, the query would be written like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100001 Man City Bike\n",
      "100002 Woman City Bike\n",
      "100003 Man City Bike\n",
      "100004 Woman City Bike\n",
      "100005 Man City Bike\n",
      "100006 Woman City Bike\n",
      "100007 Man City Bike\n",
      "100008 Woman City Bike\n",
      "100011 Man Trekking Bike\n",
      "100012 Woman Trekking Bike\n",
      "100013 Man Trekking Bike\n",
      "100014 Woman Trekking Bike\n",
      "100015 Man Trekking Bike\n",
      "100016 Woman Trekking Bike\n",
      "100017 Man Trekking Bike\n",
      "100018 Woman Trekking Bike\n",
      "100021 Mountainbike\n",
      "100022 Mountainbike\n",
      "100023 Mountainbike\n"
     ]
    }
   ],
   "source": [
    "for r in db.article.find({\"price\": {\"$gt\": 600.0}}):\n",
    "    print(r[\"_id\"], r[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple and great overview of the different commands used in NoSQL queries [can be found here](https://info-mongodb-com.s3.amazonaws.com/ReferenceCards15-PDF.pdf)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#782769'>Now it's your turn!<br>TODO #6:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Redo the first SQL Query from TODO#3.1 in NoSQL:\n",
    "#  \"Show me all customers, that are situated in Munich!\"\n",
    "#  And compare your results.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
